{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter NB has all of the data preprocessing and modeling for the Team of the Season Predictor. In the Notebook, I will go through data preprocessing, various ML arcitectures that might work for this Classification problem, stastical comparisons between the models, and a final conclusion on what model I will use and why I chose it. Some more insight into the data can be found in WebScrape Juptyer NB found in the Practice folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part of this code is from `Webscrape.ipynb`in the Practice folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"IMPORTS\"\"\"\n",
    "# Data Preprocessing\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Web Scraping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from io import StringIO\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://fbref.com/en/comps/9/stats/Premier-League-Stats\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_x/czhgmpkn0lx4yd57n3w9bwv00000gn/T/ipykernel_84267/2674133702.py:23: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  temp = pd.read_html(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No tables found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 23\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(link)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# temp = pd.read_html(\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#     requests.get(link).text.replace('<!--','').replace('-->','')\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#     ,attrs={'id':'stats_standard'}\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# )[0]\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m temp \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_html\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlink\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstats_standard\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     27\u001b[0m temp_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(temp)\n\u001b[1;32m     29\u001b[0m player_stat_dfs\u001b[38;5;241m.\u001b[39mappend(temp_data)\n",
      "File \u001b[0;32m~/anaconda3/envs/PersonalProject/lib/python3.11/site-packages/pandas/io/html.py:1240\u001b[0m, in \u001b[0;36mread_html\u001b[0;34m(io, match, flavor, header, index_col, skiprows, attrs, parse_dates, thousands, encoding, decimal, converters, na_values, keep_default_na, displayed_only, extract_links, dtype_backend, storage_options)\u001b[0m\n\u001b[1;32m   1224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[1;32m   1225\u001b[0m     [\n\u001b[1;32m   1226\u001b[0m         is_file_like(io),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1230\u001b[0m     ]\n\u001b[1;32m   1231\u001b[0m ):\n\u001b[1;32m   1232\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1233\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing literal html to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread_html\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1234\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill be removed in a future version. To read from a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1237\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m   1238\u001b[0m     )\n\u001b[0;32m-> 1240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflavor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflavor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthousands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mna_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplayed_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisplayed_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextract_links\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextract_links\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/PersonalProject/lib/python3.11/site-packages/pandas/io/html.py:1003\u001b[0m, in \u001b[0;36m_parse\u001b[0;34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1002\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m retained \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# for mypy\u001b[39;00m\n\u001b[0;32m-> 1003\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m retained\n\u001b[1;32m   1005\u001b[0m ret \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tables:\n",
      "File \u001b[0;32m~/anaconda3/envs/PersonalProject/lib/python3.11/site-packages/pandas/io/html.py:983\u001b[0m, in \u001b[0;36m_parse\u001b[0;34m(flavor, io, match, attrs, encoding, displayed_only, extract_links, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    972\u001b[0m p \u001b[38;5;241m=\u001b[39m parser(\n\u001b[1;32m    973\u001b[0m     io,\n\u001b[1;32m    974\u001b[0m     compiled_match,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    979\u001b[0m     storage_options,\n\u001b[1;32m    980\u001b[0m )\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 983\u001b[0m     tables \u001b[38;5;241m=\u001b[39m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m caught:\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;66;03m# if `io` is an io-like object, check if it's seekable\u001b[39;00m\n\u001b[1;32m    986\u001b[0m     \u001b[38;5;66;03m# and try to rewind it before trying the next parser\u001b[39;00m\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(io, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseekable\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m io\u001b[38;5;241m.\u001b[39mseekable():\n",
      "File \u001b[0;32m~/anaconda3/envs/PersonalProject/lib/python3.11/site-packages/pandas/io/html.py:249\u001b[0m, in \u001b[0;36m_HtmlFrameParser.parse_tables\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_tables\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    242\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03m    Parse and return all tables from the DOM.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;124;03m    list of parsed (header, body, footer) tuples from tables.\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m     tables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_tables\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_thead_tbody_tfoot(table) \u001b[38;5;28;01mfor\u001b[39;00m table \u001b[38;5;129;01min\u001b[39;00m tables)\n",
      "File \u001b[0;32m~/anaconda3/envs/PersonalProject/lib/python3.11/site-packages/pandas/io/html.py:598\u001b[0m, in \u001b[0;36m_BeautifulSoupHtml5LibFrameParser._parse_tables\u001b[0;34m(self, document, match, attrs)\u001b[0m\n\u001b[1;32m    596\u001b[0m tables \u001b[38;5;241m=\u001b[39m document\u001b[38;5;241m.\u001b[39mfind_all(element_name, attrs\u001b[38;5;241m=\u001b[39mattrs)\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tables:\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo tables found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    600\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    601\u001b[0m unique_tables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: No tables found"
     ]
    }
   ],
   "source": [
    "\"\"\" WEBSCRAPING PLAYER STATS \"\"\"\n",
    "\n",
    "# Player Stats from FBREF\n",
    "url_2023_2024 = 'https://fbref.com/en/comps/9/stats/Premier-League-Stats'\n",
    "url_2022_2023 = 'https://fbref.com/en/comps/9/2022-2023/stats/2022-2023-Premier-League-Stats'\n",
    "url_2021_2022 = 'https://fbref.com/en/comps/9/2021-2022/stats/2021-2022-Premier-League-Stats'\n",
    "url_2020_2021 = 'https://fbref.com/en/comps/9/2020-2021/stats/2020-2021-Premier-League-Stats'\n",
    "url_2019_2020 = 'https://fbref.com/en/comps/9/2019-2020/stats/2019-2020-Premier-League-Stats'\n",
    "url_2018_2019 = 'https://fbref.com/en/comps/9/2018-2019/stats/2018-2019-Premier-League-Stats'\n",
    "url_2017_2018 = 'https://fbref.com/en/comps/9/2017-2018/stats/2017-2018-Premier-League-Stats'\n",
    "\n",
    "urls = [ url_2023_2024 , url_2022_2023 , url_2021_2022 , url_2020_2021 , url_2019_2020, url_2018_2019, url_2017_2018]\n",
    "player_stat_dfs = []\n",
    "\n",
    "\n",
    "for link in urls:\n",
    "    print(link)\n",
    "    temp = pd.read_html(\n",
    "        requests.get(link).text.replace('<!--','').replace('-->','')\n",
    "        ,attrs={'id':'stats_standard'}\n",
    "    )[0]\n",
    "    \n",
    "    # temp = pd.read_html(\n",
    "    #     requests.get(link).text ,  attrs={'id':'stats_standard'}\n",
    "    # )[0]\n",
    "    \n",
    "    temp_data = pd.DataFrame(temp)\n",
    "    \n",
    "    player_stat_dfs.append(temp_data)\n",
    "    \n",
    "\n",
    "\n",
    "\"\"\"PULLING TEAM OF THE SEASON INFO\"\"\"\n",
    "\"\"\"TARGET SEASONS: 2017-2018 - 2023-2024\"\"\"\n",
    "\n",
    "\n",
    "# Get URLS for TOTS info\n",
    "url_tots_wiki_2020s = 'https://en.wikipedia.org/wiki/PFA_Team_of_the_Year_(2020s)'\n",
    "url_tots_wiki_2010s = 'https://en.wikipedia.org/wiki/PFA_Team_of_the_Year_(2010s)'\n",
    "\n",
    "tots_urls = [url_tots_wiki_2020s , url_tots_wiki_2010s]\n",
    "tots_dfs = []\n",
    "\n",
    "for url in tots_urls:\n",
    "    \n",
    "    # Send an HTTP GET request to the URL\n",
    "    temp_response = requests.get(url)\n",
    "    \n",
    "    if temp_response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(temp_response.content, 'html.parser')\n",
    "        \n",
    "        # Find Where all the PL Headers are \n",
    "        h4_headers = soup.find_all('h4')\n",
    "        \n",
    "        # Get PL Tags\n",
    "        premier_league_tags = [tag for tag in h4_headers if 'Premier League' in tag.text]\n",
    "        \n",
    "        \n",
    "        \n",
    "        for header in premier_league_tags:\n",
    "            # Find the next sibling table tag\n",
    "            table = header.find_next_sibling('table')\n",
    "            \n",
    "            # If a table is found, print it\n",
    "            if table:\n",
    "                df = pd.read_html(StringIO(str(table)))[0]\n",
    "                tots_dfs.append(df) \n",
    "    \n",
    "    else:\n",
    "        print(\"Failed to retrieve the page. Status code:\", temp_response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"PUT TABLES INTO DICTIONARY BY YEAR\"\"\"\n",
    "\n",
    "yr_playerStat_dict = {\n",
    "    '2023-2024' : player_stat_dfs[0],\n",
    "    '2022-2023' : player_stat_dfs[1],\n",
    "    '2021-2022' : player_stat_dfs[2],\n",
    "    '2020-2021' : player_stat_dfs[3],\n",
    "    '2019-2020' : player_stat_dfs[4],\n",
    "    '2018-2019' : player_stat_dfs[5],\n",
    "    '2017-2018' : player_stat_dfs[6]\n",
    "}\n",
    "\n",
    "\"\"\"MAPPING TABLES TO APROPRIATE YEAR IN DICTIONARY (year -> TOTS Table)\"\"\"\n",
    "\n",
    "yr_totsNoms_dict = {\n",
    "    '2019-2020' : tots_dfs[0] ,\n",
    "    '2020-2021' : tots_dfs[1] ,\n",
    "    '2021-2022' : tots_dfs[2] ,\n",
    "    '2022-2023' : tots_dfs[3] , \n",
    "    '2017-2018' : tots_dfs[-2] , \n",
    "    '2018-2019' : tots_dfs[-1] , \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_seasons = ['2017-2018' , '2018-2019' , '2019-2020' , '2020-2021', '2021-2022' , '2022-2023'] # Exclude current Season\n",
    "\n",
    "# All DataFrames Stored\n",
    "merged_dataframes = []\n",
    "\n",
    "for szn in list_seasons:\n",
    "    tmp_tots_stats = yr_totsNoms_dict[szn]\n",
    "    tmp_plyr_stats = yr_playerStat_dict[szn]\n",
    "    \n",
    "    # Strip Player name to get rid of special chars\n",
    "    tmp_tots_stats['Player'] = tmp_tots_stats['Player'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x).strip())\n",
    "    \n",
    "    # Updating Names for TOTS Nominees Table to match Player Stats DF\n",
    "    tmp_tots_stats = tmp_tots_stats.rename(columns={'App.': 'Tots_Apps' , 'Pos.': 'Pos' , 'Club': 'Squad'})\n",
    "    \n",
    "    # Drop Upper Level Layers\n",
    "    tmp_plyr_stats.columns = tmp_plyr_stats.columns.droplevel([0])\n",
    "    \n",
    "    # Merge the DataFrames\n",
    "    # Join tables, keep all of info from player stats , join on Player name and squad\n",
    "    tmp_df_merged = pd.merge(tmp_plyr_stats, tmp_tots_stats, on=['Player', 'Squad'], how='left')\n",
    "    \n",
    "    #Fix Pos_x \n",
    "    tmp_df_merged['Pos'] = tmp_df_merged['Pos_x'].str.split(',').str[0]\n",
    "    \n",
    "    # If Tots_Apps = NaN\n",
    "    # Make that = 0\n",
    "    tmp_df_merged['Tots_Apps'] = tmp_df_merged['Tots_Apps'].fillna(0)\n",
    "    \n",
    "    # Nation \n",
    "    # Use the ALL CAPS Countries\n",
    "    tmp_df_merged['Nation'] = tmp_df_merged['Nation'].str.split().str[-1]\n",
    "    \n",
    "    # Drop Matches, Rk , Extra Pos's\n",
    "    tmp_df_merged = tmp_df_merged.drop(columns=['Rk', 'Matches' , 'Pos_x' , 'Pos_y' ])\n",
    "    \n",
    "    merged_dataframes.append(tmp_df_merged)\n",
    "    \n",
    "    \n",
    "for mdf in merged_dataframes: mdf.drop(mdf[mdf['Player'] == 'Player'].index, inplace=True)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(merged_dataframes))\n",
    "merged_dataframes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in the target column\n",
    "for mdf in merged_dataframes:\n",
    "    mdf['TOTS'] = mdf['Tots_Apps'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataframes[4].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_plyrs = 0\n",
    "tots_plyrs = 11 * len(merged_dataframes)\n",
    "for merged_df in merged_dataframes:\n",
    "    total_plyrs += len(merged_df)\n",
    "\n",
    "print(f\"Players in TOTS over 6 years: { tots_plyrs }\")\n",
    "print(f\"Players with stats over 6 years: { total_plyrs }\")\n",
    "print(f\"Percentage of players that are in TOTS: % {tots_plyrs /total_plyrs * 100 }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the code above, we can see that we have major class imbalance when putting the DataFrames together. We will see how the model works out and if it performs well when the data is put together across different seasons. \n",
    "\n",
    "The reasoning behind putting the data together is that I am hoping to find a consistent standard on certain stats that lead to a player being nominated for TOTS. If I can establish this relationship, then I can use the stats for a players current season, find out what they average to, find an expected total, and use that to predict if they will make TOTS or not.\n",
    "\n",
    "For this predict method we will have to look into Fbeta scores and see how our model performs using a confusion matrix. I also need to establish if False Positives or False Positives are more important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"JOINING THE TABLES\"\"\"\n",
    "df = pd.concat(merged_dataframes)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to make the 'Squad' column into a ordinal categorical variable by ordering the squads by average league position over the time we are getting the data. We are doing this because when looking at TOTS data, it is evident that teams that place higher in the standings tend to have more players in the TOTS. This is a big assumption I am making, so if it does not seem to have an affect on what we get from the model, I will find another way to handle this.\n",
    "\n",
    "To accomplish this, we need to do more webscraping from fbref to find the tables and get the average position for each team that played in the Premier League in the timeframe from the 2017-18 season to the 2022-2023 season. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_seasons = ['2017-2018' , '2018-2019' , '2019-2020' , '2020-2021', '2021-2022' , '2022-2023'] # Exclude current Season\n",
    "\n",
    "#     # url_temp = f'https://fbref.com/en/comps/9/{szn}/stats/{szn}-Premier-League-Stats'\n",
    "# url_temp = f'https://fbref.com/en/comps/9/2017-2018/stats/2017-2018-Premier-League-Stats'\n",
    "\n",
    "# # https://fbref.com/en/comps/9/2017-2018/2017-2018-Premier-League-Stats\n",
    "\n",
    "# for szn in list_seasons:\n",
    "#     url_temp = f'https://fbref.com/en/comps/9/{szn}/{szn}-Premier-League-Stats'\n",
    "#     print(url_temp)\n",
    "#     # Send an HTTP GET request to the URL\n",
    "#     temp_response = requests.get(url_temp)\n",
    "    \n",
    "#     if temp_response.status_code == 200:\n",
    "#         # Parse the HTML content\n",
    "#         soup = BeautifulSoup(temp_response.content, 'html.parser')\n",
    "        \n",
    "#         # Find Where all the Regular Season Headers are \n",
    "#         h2_headers = soup.find_all('h2')\n",
    "        \n",
    "#         # Get PL Tags\n",
    "#         standings_tags = [tag for tag in h2_headers if 'Regular season Table' in tag.text]\n",
    "#         print(standings_tags)\n",
    "        \n",
    "#         for header in premier_league_tags:\n",
    "#             # Find the next sibling table tag\n",
    "#             table = header.find_next_sibling('table')\n",
    "            \n",
    "#             # If a table is found, print it\n",
    "#             if table:\n",
    "#                 df = pd.read_html(StringIO(str(table)))[0]\n",
    "#                 tots_dfs.append(df) \n",
    "    \n",
    "#     else:\n",
    "#         print(\"Failed to retrieve the page. Status code:\", temp_response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PersonalProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
